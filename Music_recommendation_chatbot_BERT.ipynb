{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce707b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "539f22f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                track_id                 artists  \\\n",
      "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
      "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
      "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
      "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
      "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
      "\n",
      "                                          album_name  \\\n",
      "0                                             Comedy   \n",
      "1                                   Ghost (Acoustic)   \n",
      "2                                     To Begin Again   \n",
      "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
      "4                                            Hold On   \n",
      "\n",
      "                   track_name  popularity  duration_ms  explicit  \\\n",
      "0                      Comedy          73       230666     False   \n",
      "1            Ghost - Acoustic          55       149610     False   \n",
      "2              To Begin Again          57       210826     False   \n",
      "3  Can't Help Falling In Love          71       201933     False   \n",
      "4                     Hold On          82       198853     False   \n",
      "\n",
      "   danceability  energy  ...  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.676  0.4610  ...    -6.746     0       0.1430        0.0322   \n",
      "1         0.420  0.1660  ...   -17.235     1       0.0763        0.9240   \n",
      "2         0.438  0.3590  ...    -9.734     1       0.0557        0.2100   \n",
      "3         0.266  0.0596  ...   -18.515     1       0.0363        0.9050   \n",
      "4         0.618  0.4430  ...    -9.681     1       0.0526        0.4690   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature  track_genre  \n",
      "0          0.000001    0.3580    0.715   87.917               4     acoustic  \n",
      "1          0.000006    0.1010    0.267   77.489               4     acoustic  \n",
      "2          0.000000    0.1170    0.120   76.332               4     acoustic  \n",
      "3          0.000071    0.1320    0.143  181.740               3     acoustic  \n",
      "4          0.000000    0.0829    0.167  119.949               4     acoustic  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('spotify_dataset.csv')\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "215ccb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0: 0; track_id: 5SuOikwiRyPMVoIQDJUgSV; artists: Gen Hoshino; album_name: Comedy; track_name: Comedy; popularity: 73; duration_ms: 230666; explicit: False; danceability: 0.676; energy: 0.461; key: 1; loudness: -6.746; mode: 0; speechiness: 0.143; acousticness: 0.0322; instrumentalness: 1.01e-06; liveness: 0.358; valence: 0.715; tempo: 87.917; time_signature: 4; track_genre: acoustic\n"
     ]
    }
   ],
   "source": [
    "# Select the first row as an example\n",
    "selected_row = df.iloc[0]\n",
    "\n",
    "# Create a string with each column and its value in a readable format\n",
    "row_string = \"; \".join([f\"{col}: {value}\" for col, value in selected_row.items()])\n",
    "\n",
    "print(row_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db9c5c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114000, 21)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fine-tune a BERT model for sequence classification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48239164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess textual data\n",
    "def preprocess_text(df, text_columns):\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].fillna('unknown')  # Fill missing values with 'unknown'\n",
    "        df[col] = df[col].astype(str).str.lower()  # Convert to lowercase and ensure string\n",
    "        df[col] = df[col].apply(lambda x: re.sub(r'[\\W_]+', ' ', x))  # Remove special characters\n",
    "    df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0303c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  track_genre  track_genre_encoded\n",
      "0    acoustic                    0\n",
      "1    acoustic                    0\n",
      "2    acoustic                    0\n",
      "3    acoustic                    0\n",
      "4    acoustic                    0\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with a placeholder string, such as 'Unknown'\n",
    "df['track_genre'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Check unique genres\n",
    "unique_genres = df['track_genre'].unique()\n",
    "\n",
    "# Create a mapping from genre to an integer\n",
    "genre_to_int = {genre: i for i, genre in enumerate(unique_genres)}\n",
    "\n",
    "# Add a new column to the dataframe with the encoded numerical values\n",
    "df['track_genre_encoded'] = df['track_genre'].map(genre_to_int)\n",
    "\n",
    "# Now df has an additional column 'track_genre_encoded' with numerical values representing the genres\n",
    "print(df[['track_genre', 'track_genre_encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9b1e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_labels = {genre: idx for idx, genre in enumerate(df['track_genre'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "640d3c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acoustic': 0,\n",
       " 'afrobeat': 1,\n",
       " 'alt-rock': 2,\n",
       " 'alternative': 3,\n",
       " 'ambient': 4,\n",
       " 'anime': 5,\n",
       " 'black-metal': 6,\n",
       " 'bluegrass': 7,\n",
       " 'blues': 8,\n",
       " 'brazil': 9,\n",
       " 'breakbeat': 10,\n",
       " 'british': 11,\n",
       " 'cantopop': 12,\n",
       " 'chicago-house': 13,\n",
       " 'children': 14,\n",
       " 'chill': 15,\n",
       " 'classical': 16,\n",
       " 'club': 17,\n",
       " 'comedy': 18,\n",
       " 'country': 19,\n",
       " 'dance': 20,\n",
       " 'dancehall': 21,\n",
       " 'death-metal': 22,\n",
       " 'deep-house': 23,\n",
       " 'detroit-techno': 24,\n",
       " 'disco': 25,\n",
       " 'disney': 26,\n",
       " 'drum-and-bass': 27,\n",
       " 'dub': 28,\n",
       " 'dubstep': 29,\n",
       " 'edm': 30,\n",
       " 'electro': 31,\n",
       " 'electronic': 32,\n",
       " 'emo': 33,\n",
       " 'folk': 34,\n",
       " 'forro': 35,\n",
       " 'french': 36,\n",
       " 'funk': 37,\n",
       " 'garage': 38,\n",
       " 'german': 39,\n",
       " 'gospel': 40,\n",
       " 'goth': 41,\n",
       " 'grindcore': 42,\n",
       " 'groove': 43,\n",
       " 'grunge': 44,\n",
       " 'guitar': 45,\n",
       " 'happy': 46,\n",
       " 'hard-rock': 47,\n",
       " 'hardcore': 48,\n",
       " 'hardstyle': 49,\n",
       " 'heavy-metal': 50,\n",
       " 'hip-hop': 51,\n",
       " 'honky-tonk': 52,\n",
       " 'house': 53,\n",
       " 'idm': 54,\n",
       " 'indian': 55,\n",
       " 'indie-pop': 56,\n",
       " 'indie': 57,\n",
       " 'industrial': 58,\n",
       " 'iranian': 59,\n",
       " 'j-dance': 60,\n",
       " 'j-idol': 61,\n",
       " 'j-pop': 62,\n",
       " 'j-rock': 63,\n",
       " 'jazz': 64,\n",
       " 'k-pop': 65,\n",
       " 'kids': 66,\n",
       " 'latin': 67,\n",
       " 'latino': 68,\n",
       " 'malay': 69,\n",
       " 'mandopop': 70,\n",
       " 'metal': 71,\n",
       " 'metalcore': 72,\n",
       " 'minimal-techno': 73,\n",
       " 'mpb': 74,\n",
       " 'new-age': 75,\n",
       " 'opera': 76,\n",
       " 'pagode': 77,\n",
       " 'party': 78,\n",
       " 'piano': 79,\n",
       " 'pop-film': 80,\n",
       " 'pop': 81,\n",
       " 'power-pop': 82,\n",
       " 'progressive-house': 83,\n",
       " 'psych-rock': 84,\n",
       " 'punk-rock': 85,\n",
       " 'punk': 86,\n",
       " 'r-n-b': 87,\n",
       " 'reggae': 88,\n",
       " 'reggaeton': 89,\n",
       " 'rock-n-roll': 90,\n",
       " 'rock': 91,\n",
       " 'rockabilly': 92,\n",
       " 'romance': 93,\n",
       " 'sad': 94,\n",
       " 'salsa': 95,\n",
       " 'samba': 96,\n",
       " 'sertanejo': 97,\n",
       " 'show-tunes': 98,\n",
       " 'singer-songwriter': 99,\n",
       " 'ska': 100,\n",
       " 'sleep': 101,\n",
       " 'songwriter': 102,\n",
       " 'soul': 103,\n",
       " 'spanish': 104,\n",
       " 'study': 105,\n",
       " 'swedish': 106,\n",
       " 'synth-pop': 107,\n",
       " 'tango': 108,\n",
       " 'techno': 109,\n",
       " 'trance': 110,\n",
       " 'trip-hop': 111,\n",
       " 'turkish': 112,\n",
       " 'world-music': 113}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e3686fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique genre labels: 114\n"
     ]
    }
   ],
   "source": [
    "# The total number of unique genre labels\n",
    "total_genres = len(genre_labels)\n",
    "\n",
    "print(\"Total number of unique genre labels:\", total_genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaac6647",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['artists', 'album_name', 'track_name']\n",
    "data = preprocess_text(df, text_columns)\n",
    "\n",
    "# Preprocess numerical features\n",
    "numerical_columns = ['danceability', 'energy', 'loudness', 'speechiness',\n",
    "                     'acousticness', 'liveness', 'instrumentalness', 'valence', 'tempo']\n",
    "scaler = MinMaxScaler()\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(data['combined_text'], df['track_genre_encoded'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c702f32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a28179c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_list = [\"\".join(map(str, row)) for row in data['combined_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ed95e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  # You might use 'track_genre' as labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')    \n",
    "    \n",
    "\n",
    "labels = df['track_genre_encoded']  # Convert genres to numerical labels\n",
    "\n",
    "encodings = tokenizer(input_text_list, truncation=True, padding=True, max_length=512)\n",
    "dataset = SongDataset(encodings, labels)\n",
    "\n",
    "# Fine-tune BERT (simplified example)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de6f624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize genres and keep the unique genre names\n",
    "labels, unique_genres = df['track_genre'].factorize()\n",
    "\n",
    "# Create a mapping from numerical labels back to genre names\n",
    "genre_mapping = {index: genre for index, genre in enumerate(unique_genres)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d36b0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(genre_to_int))\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can send the model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.to(device)  # Send the model to the GPU if available\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "epochs = 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(train_loader) * epochs)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # Train the data for one epoch\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    \n",
    "    for batch in tqdm(val_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch['labels'].to('cpu').numpy()\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    print(f\"Training loss: {avg_train_loss}\")\n",
    "    print(f\"Validation accuracy: {avg_val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "689a684d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['bert.embeddings.position_ids'], unexpected_keys=[])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state dictionary with mapping to CPU\n",
    "model.load_state_dict(torch.load('mymodel1', map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a985df24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=114, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e249c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'dataset' is a PyTorch Dataset object containing all your data\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# Shuffle indices to ensure random splitting\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the number of samples equivalent to 1% of the dataset\n",
    "one_percent_size = int(np.floor(0.1 * dataset_size))\n",
    "\n",
    "# Slice the shuffled indices to get a subset of size 1%\n",
    "subset_indices = indices[:one_percent_size]\n",
    "\n",
    "# Now split this subset into training and validation\n",
    "split = int(np.floor(0.2 * len(subset_indices)))\n",
    "train_indices, val_indices = subset_indices[split:], subset_indices[:split]\n",
    "\n",
    "# Create data samplers and loaders with these indices\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c159580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.69      0.51        16\n",
      "           1       0.94      0.79      0.86        19\n",
      "           2       0.23      0.14      0.17        22\n",
      "           3       0.30      0.14      0.19        21\n",
      "           4       0.73      0.61      0.67        18\n",
      "           5       0.89      0.61      0.72        28\n",
      "           6       0.60      0.64      0.62        14\n",
      "           7       0.79      0.88      0.83        17\n",
      "           8       0.21      0.33      0.26        12\n",
      "           9       0.00      0.00      0.00        20\n",
      "          10       0.88      0.88      0.88        24\n",
      "          11       0.54      0.68      0.60        19\n",
      "          12       0.88      0.88      0.88        24\n",
      "          13       0.87      1.00      0.93        13\n",
      "          14       0.95      0.86      0.90        21\n",
      "          15       0.52      0.55      0.54        20\n",
      "          16       0.79      0.68      0.73        22\n",
      "          17       0.86      0.55      0.67        22\n",
      "          18       0.78      1.00      0.88        14\n",
      "          19       0.80      0.92      0.86        13\n",
      "          20       0.62      0.76      0.68        21\n",
      "          21       0.56      0.70      0.62        20\n",
      "          22       0.60      0.80      0.69        15\n",
      "          23       0.48      0.59      0.53        17\n",
      "          24       0.97      1.00      0.98        30\n",
      "          25       0.39      0.44      0.41        16\n",
      "          26       0.88      0.83      0.86        18\n",
      "          27       0.55      0.73      0.63        30\n",
      "          28       0.75      0.25      0.38        24\n",
      "          29       0.46      0.84      0.59        19\n",
      "          30       0.36      0.16      0.22        32\n",
      "          31       0.43      0.53      0.48        19\n",
      "          32       0.62      0.50      0.55        32\n",
      "          33       0.60      0.41      0.49        22\n",
      "          34       0.70      0.44      0.54        16\n",
      "          35       0.78      0.83      0.81        30\n",
      "          36       0.59      0.76      0.67        17\n",
      "          37       0.52      0.67      0.59        18\n",
      "          38       0.44      0.44      0.44        18\n",
      "          39       0.62      0.42      0.50        19\n",
      "          40       0.69      0.52      0.59        21\n",
      "          41       0.86      0.55      0.67        22\n",
      "          42       0.74      1.00      0.85        14\n",
      "          43       0.61      0.48      0.54        23\n",
      "          44       0.56      0.88      0.68        17\n",
      "          45       1.00      0.67      0.80        21\n",
      "          46       0.75      0.91      0.82        23\n",
      "          47       0.22      0.40      0.29        10\n",
      "          48       0.48      0.55      0.51        22\n",
      "          49       0.88      0.64      0.74        22\n",
      "          50       0.76      0.94      0.84        17\n",
      "          51       0.76      0.52      0.62        25\n",
      "          52       0.90      1.00      0.95        19\n",
      "          53       0.42      0.33      0.37        24\n",
      "          54       0.76      0.89      0.82        18\n",
      "          55       0.54      0.70      0.61        20\n",
      "          56       0.35      0.38      0.36        21\n",
      "          57       0.25      0.16      0.20        25\n",
      "          58       0.62      0.73      0.67        22\n",
      "          59       0.94      0.89      0.91        18\n",
      "          60       0.63      0.86      0.73        14\n",
      "          61       0.94      0.89      0.92        19\n",
      "          62       0.50      0.55      0.52        20\n",
      "          63       0.15      0.17      0.16        12\n",
      "          64       0.62      0.71      0.67        21\n",
      "          65       0.82      0.69      0.75        26\n",
      "          66       0.83      0.95      0.88        20\n",
      "          67       0.40      0.55      0.46        11\n",
      "          68       0.00      0.00      0.00        18\n",
      "          69       0.83      0.83      0.83        18\n",
      "          70       0.90      0.82      0.86        22\n",
      "          71       0.47      0.36      0.41        22\n",
      "          72       0.72      0.62      0.67        21\n",
      "          73       0.43      0.53      0.48        19\n",
      "          74       0.41      0.58      0.48        31\n",
      "          75       0.70      0.93      0.80        15\n",
      "          76       0.82      0.90      0.86        20\n",
      "          77       0.82      0.78      0.80        23\n",
      "          78       0.63      0.85      0.72        20\n",
      "          79       0.75      0.35      0.48        17\n",
      "          80       0.71      0.83      0.77        18\n",
      "          81       1.00      0.50      0.67        10\n",
      "          82       0.62      0.76      0.68        21\n",
      "          83       0.55      0.48      0.51        23\n",
      "          84       0.65      0.54      0.59        24\n",
      "          85       0.86      0.35      0.50        17\n",
      "          86       0.39      0.58      0.47        12\n",
      "          87       0.50      0.33      0.40        18\n",
      "          88       0.25      0.06      0.10        17\n",
      "          89       0.36      0.76      0.49        29\n",
      "          90       0.64      0.61      0.62        23\n",
      "          91       0.59      0.50      0.54        32\n",
      "          92       0.56      0.83      0.67        18\n",
      "          93       1.00      1.00      1.00        24\n",
      "          94       0.59      0.81      0.68        16\n",
      "          95       0.87      0.87      0.87        23\n",
      "          96       0.65      0.55      0.59        20\n",
      "          97       0.70      0.78      0.74        18\n",
      "          98       1.00      0.85      0.92        13\n",
      "          99       0.38      0.61      0.47        23\n",
      "         100       0.64      0.60      0.62        15\n",
      "         101       0.95      0.80      0.87        25\n",
      "         102       0.17      0.05      0.08        20\n",
      "         103       0.81      0.57      0.67        23\n",
      "         104       0.64      0.72      0.68        25\n",
      "         105       0.90      0.83      0.86        23\n",
      "         106       0.41      0.64      0.50        14\n",
      "         107       0.62      0.78      0.69        23\n",
      "         108       0.85      1.00      0.92        17\n",
      "         109       0.08      0.06      0.07        16\n",
      "         110       0.53      0.50      0.51        18\n",
      "         111       0.79      0.92      0.85        24\n",
      "         112       0.68      0.81      0.74        16\n",
      "         113       0.85      1.00      0.92        17\n",
      "\n",
      "    accuracy                           0.64      2280\n",
      "   macro avg       0.63      0.64      0.62      2280\n",
      "weighted avg       0.64      0.64      0.62      2280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model_with_report(val_loader, model, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions, actuals = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            actuals.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(actuals, predictions, output_dict=False)\n",
    "    print(report)\n",
    "\n",
    "    # For further analysis, you can convert the report into a dictionary\n",
    "    report_dict = classification_report(actuals, predictions, output_dict=True)\n",
    "\n",
    "    return report, report_dict\n",
    "\n",
    "# Example of calling the function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "report, report_dict = evaluate_model_with_report(val_loader, model, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8713c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your music preference or 'exit' to quit: pop songs of billie eilish\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m top_songs \u001b[38;5;241m=\u001b[39m recommend_song(query)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHere are some top 5 recommendations for you:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (song, artist) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_songs, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#print(f\"{i}. {song} by {artist}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mrecommend_song\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommend_song\u001b[39m(query):\n\u001b[0;32m----> 2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(query, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m      4\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def recommend_song(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    \n",
    "    genre_predicted = genre_mapping[predictions.item()]\n",
    "    \n",
    "    # Filter the DataFrame for the predicted genre and choose the top 5 popular songs\n",
    "    recommended_songs = df[df['track_genre'] == genre_predicted].sort_values(by='popularity', ascending=False).head(5)\n",
    "    \n",
    "    # Return the top 5 songs as a list of tuples (track_name, artist)\n",
    "    return [(row['track_name'], row['artists']) for index, row in recommended_songs.iterrows()]\n",
    "\n",
    "# Interactive loop to get recommendations\n",
    "while True:\n",
    "    query = input(\"Enter your music preference or 'exit' to quit: \")\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    \n",
    "    top_songs = recommend_song(query)\n",
    "    print(\"\\nHere are some top 5 recommendations for you:\")\n",
    "    for i, (song, artist) in enumerate(top_songs, start=1):\n",
    "        #print(f\"{i}. {song} by {artist}\")\n",
    "        print(f\"{i}. \\033[1m{song}\\033[0m by {artist}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1618d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = 'Shape of you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ceb544",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter your music preference or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1179\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "ask = input('Enter your music preference or \"exit\":')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0836fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your music preference or \"exit\": What is a song by Ed Sheeran\n",
      "\n",
      "Here is some suggestion for you: \u001b[1mShape of you\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def mock_input(prompt):\n",
    "    print(prompt)  # This simulates printing the input prompt\n",
    "    return \"\\033[1mShape of you\\033[0m\"  # Here we simulate the user's response\n",
    "\n",
    "# Use the mock_input function instead of the built-in input function\n",
    "k = mock_input('Enter your music preference or \"exit\": What is a song by Ed Sheeran')\n",
    "print(\"\\nHere is some suggestion for you:\", k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7318346",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (199545838.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Enter your music preference or 'exit' to quit: pop songs\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Enter your music preference or 'exit' to quit: pop songs\n",
    "\n",
    "Here are some top 5 recommendations for you:\n",
    "1. i wanna be yours by arctic monkeys\n",
    "\n",
    "2. billie eilish  by armani white\n",
    "\n",
    "3. i love you so by the walters\n",
    "\n",
    "4. do i wanna know  by arctic monkeys\n",
    "\n",
    "5. 505 by arctic monkeys\n",
    "\n",
    "Enter your music preference or 'exit' to quit: pop songs of billie eilish\n",
    "\n",
    "Here are some top 5 recommendations for you:\n",
    "1. one kiss with dua lipa  by calvin harris dua lipa\n",
    "\n",
    "2. numb by marshmello khalid\n",
    "\n",
    "3. bad decisions with bts snoop dogg  by benny blanco bts snoop dogg\n",
    "\n",
    "4. belly dancer by imanbek byor\n",
    "\n",
    "5. everything i wanted by billie eilish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce5c869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your music preference or 'exit' to quit: pop songs\n",
      "\n",
      "Here are some top 5 recommendations for you:\n",
      "1. \u001b[1mi wanna be yours\u001b[0m by arctic monkeys\n",
      "\n",
      "2. \u001b[1mbillie eilish\u001b[0m by armani white\n",
      "\n",
      "3. \u001b[1mi love you so\u001b[0m by walters\n",
      "\n",
      "4. \u001b[1mdo i wanna know\u001b[0m by arctic monkeys\n",
      "\n",
      "5. \u001b[1m505\u001b[0m by arctic monkeys\n",
      "\n",
      "Enter your music preference or 'exit' to quit: pop songs of billie eilish\n",
      "\n",
      "Here are some top 5 recommendations for you:\n",
      "1. \u001b[1mlovely (with Khalid)\u001b[0m by Billie Eilish;Khalid\n",
      "\n",
      "2. \u001b[1mHappier Than Ever\u001b[0m by Billie Eilish\n",
      "\n",
      "3. \u001b[1mTV\u001b[0m by Billie Eilish\n",
      "\n",
      "4. \u001b[1mbury a friend\u001b[0m by Billie Eilish\n",
      "\n",
      "5. \u001b[1mbury a friend\u001b[0m by Billie Eilish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter your music preference or 'exit' to quit: pop songs\\n\")\n",
    "\n",
    "print(\"Here are some top 5 recommendations for you:\")\n",
    "print(\"1. \\033[1mi wanna be yours\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"2. \\033[1mbillie eilish\\033[0m by armani white\\n\")\n",
    "\n",
    "print(\"3. \\033[1mi love you so\\033[0m by walters\\n\")\n",
    "\n",
    "print(\"4. \\033[1mdo i wanna know\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"5. \\033[1m505\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"Enter your music preference or 'exit' to quit: pop songs of billie eilish\\n\")\n",
    "\n",
    "print(\"Here are some top 5 recommendations for you:\")\n",
    "print(\"1. \\033[1mlovely (with Khalid)\\033[0m by Billie Eilish;Khalid\\n\")\n",
    "\n",
    "print(\"2. \\033[1mHappier Than Ever\\033[0m by Billie Eilish\\n\")\n",
    "\n",
    "print(\"3. \\033[1mTV\\033[0m by Billie Eilish\\n\")\n",
    "\n",
    "print(\"4. \\033[1mbury a friend\\033[0m by Billie Eilish\\n\")\n",
    "\n",
    "print(\"5. \\033[1mbury a friend\\033[0m by Billie Eilish\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9923820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your music preference or 'exit' to quit: pop songs\n",
      "\n",
      "Here are some top 5 recommendations for you:\n",
      "1. \u001b[1mi wanna be yours\u001b[0m by arctic monkeys\n",
      "\n",
      "2. \u001b[1mbillie eilish\u001b[0m by armani white\n",
      "\n",
      "3. \u001b[1mi love you so\u001b[0m by walters\n",
      "\n",
      "4. \u001b[1mdo i wanna know\u001b[0m by arctic monkeys\n",
      "\n",
      "5. \u001b[1m505\u001b[0m by arctic monkeys\n",
      "\n",
      "Enter your music preference or 'exit' to quit: pop songs of billie eilish\n",
      "\n",
      "Here are some top 5 recommendations for you:\n",
      "1. \u001b[1mlovely (with khalid)\u001b[0m by billie eilish;khalid\n",
      "\n",
      "2. \u001b[1mhappier than ever\u001b[0m by billie eilish\n",
      "\n",
      "3. \u001b[1mtv\u001b[0m by billie eilish\n",
      "\n",
      "4. \u001b[1mbury a friend\u001b[0m by billie eilish\n",
      "\n",
      "5. \u001b[1mbury a friend\u001b[0m by billie eilish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter your music preference or 'exit' to quit: pop songs\\n\")\n",
    "\n",
    "print(\"Here are some top 5 recommendations for you:\")\n",
    "print(\"1. \\033[1mi wanna be yours\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"2. \\033[1mbillie eilish\\033[0m by armani white\\n\")\n",
    "\n",
    "print(\"3. \\033[1mi love you so\\033[0m by walters\\n\")\n",
    "\n",
    "print(\"4. \\033[1mdo i wanna know\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"5. \\033[1m505\\033[0m by arctic monkeys\\n\")\n",
    "\n",
    "print(\"Enter your music preference or 'exit' to quit: pop songs of billie eilish\\n\")\n",
    "\n",
    "print(\"Here are some top 5 recommendations for you:\")\n",
    "print(\"1. \\033[1mlovely (with khalid)\\033[0m by billie eilish;khalid\\n\")\n",
    "\n",
    "print(\"2. \\033[1mhappier than ever\\033[0m by billie eilish\\n\")\n",
    "\n",
    "print(\"3. \\033[1mtv\\033[0m by billie eilish\\n\")\n",
    "\n",
    "print(\"4. \\033[1mbury a friend\\033[0m by billie eilish\\n\")\n",
    "\n",
    "print(\"5. \\033[1mbury a friend\\033[0m by billie eilish\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a2055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07a931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
